# Ames Housing Predictions using a Lasso Regression Model
## by Jamie McElhiney

## Introduction

Why are people obsessed with housing prices? Why are we in awe when a mini townhouse in downtown Manhattan goes for over 15 million dollars? I mean its just a pricetag on something necessary for living a normal life. Most of us have been at that point in our life when its time to move out from our parents basent (not me!!) and start looking on StreetEasy for a place to rent or buy. And all of a sudden we're fascinated by the pre-War studio apartments in TriBeca going for 10 times the price of a luxury condo in south Brooklyn. Or, similarly, maybe its that 'Full House' house in SF compared to a beachside condo in Seal Beach that goes for a fraction of the price. 

When we look at buying or renting, a few features immediately come to mind. Square footage, number of bedrooms, and number of bathrooms. However, when we look more closesly at real estate website listings, we see a long list of features underneath these main baseline apartment stats. That list is usually named 'Amenities' and has all sorts of information about the building, more than we think we would need. Think again. What if its a super nice building, recently built, great deal (price),and has all the amenities you would ever need, but its a mile away from the closest subway and thats just a deal breaker for most people. Or maybe its its in a neighborhood with the most homicides per square mile? Alternatively, what if its a super small studio apartment in a mediocre neighborhood, but theres a roof deck, an outside area with a grill and outdoor seating. These factors clearly come into play and muddle what we think to be an accurate predictive proces of housing prices.

## Problem Statement

How can we accurately predict housing prices? We want to pose as a real estate consultant and my clients are people who want to sell their houses. Its important to give an accurate quote on house price tag since people want their house to be sold within a reasonable time frame and at a good price so they get their money's worth. Also, a lot of people make a living off of flipping houses, so being able to maximize profit in these cases is important. We are going to build a baseline model trained on the data set found on kaggle here:[Ames Housing Data](https://www.kaggle.com/c/dsi-us-10-project-2-regression-challenge/data). We will then use a RidgeCV fit to produce a more accurate model and compare and contrast their metrics. 

## Executive Summary

We are using the dataset linked above to train an advanced regression model to come to a conclusion about how to price our clients houses. We want to identify features that are have high correlation coefficients and will be persent in our model so we can advise our clients appropriately. For example our kitchen quality feature is highly correlated with Sale Price, so if a home has a below average kitchen, a renovation may be needed.   
We looked at some outliers with simple scatter plots and dug into the dataset to see if there was any indication of why their price was not in line with the data. We found nothing to indicate a reason, so they were dropped from the sit.  
We also thought about our geographic region in this case, Ames, Iowa, and determine why certain features may have high correlations and did some background research into why that may be. We used a heat map of highest correlated features with sale price to see which features stuck out. We found most of our Basement features, such as total basement square footage and basement exposure had high correlation. This is specific to Iowa because Iowa is in a tornado-heavy area and residents usually seek shelter in their basements while they wait for the tornado to pass. This is why Basement features hold more weight in our model and why we include them when developing a baseline model and moving forward.   
We ran into issues with null values and we wanted to fill them appropriately. Using the [Data Dictionary](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt), I looked at what the equivalent of a null values was for each feature. For example a null value in a continuous variable would be filled with 0. A null value in a feature where the original corresponding feature did not exist, would be filled with NA or None depending on the feature and again, referring to the data dictionary linked above. 
Some features were engineered, specifically a feature that combined full bathrooms and half bathrooms. We also made an age sold feature, the age of the house when it was sold, and a year remodeled before feature, a feature saying how long ago from the sale year was the house remodeled. If features were added or subtracted to create a new feature, they were not included in the model. 
Logarithmic transformations were applied to larger continuous variables to normalize their distribution so they could work with our model. These decision were based on our EDA histogram subplots and variables that have heavy skew were transformed. Dummy columns were created from values that could not be quantified so we could accout for neighborhoods, different types, and miscellaneous features. Any feature that COULD be quantified, their values were mapped to integers usually ranking from 1-7 or however many types/values were available. 
Root mean squared error was used to evaluate our model's accuracy. the root mean squared error is determined by equation. 
$$\text{RMSE}=\sqrt{\text{MSE}}=\sqrt{\sum_{i=1}^{n}{(\hat{y}_i-y_i)}^2}$$

We started with a basic OLS model and determine performance. We then moved to a Lasso regressio model, evaluated its performance with similar standards and proved its accuracy to our clients with the provided metrics. 

## Conclusions and Recommendations

We made a baseline model with some basic features and fit it to our training data. Our initial root mean squared error for our baseline model was 34,593.29 dollars. So, on average our predictions for our y_test values were off by a value of 34,593.29 dollars. We evaluated our baseline, see where we could improve it, and we decided on a RidgeCV model. This uses a penalty value to to nullify beta coefficients for values that arent as essential to our model. This model was chosen over Ridge, which minimizes instead of nullifys, for simplicitys sake. Having a lower number of features but still managing to have a low RMSE is an ideal end result we want to look for. So we threw in as many features as we could and allowed the penalty to assign 0 to beta coefficients of variables we didnt need. Our highest Beta coefficients were similar to high value features we identified in our heatmaps. GrLivArea, OverallQual, Total Bsmt SF.  We also applied a logarithmic transformation to our dependent variable, SalePrice, to account for the non normal distribution of SalePrice. This allows our model to handle outliers easier. Our new root mean squared error ends up at 18,233.51$, a major improvement from our baseline model. So, we would recommend using a RidgeCV model and overfitting to allow the Lasso to choose our features based on the penalty coefficient we land on. 
